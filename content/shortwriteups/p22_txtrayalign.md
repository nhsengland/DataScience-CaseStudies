---
layout: base
title: Title
permalink: p22_txtrayalign.html
---

# {{page.title}}
> | "An investigation of extracting insight from multi-modal data using contrastive learning.  This work aimed to demonstrate how to go about creating text from images for chest X-rays.  This project was directly followed by a second in this area."   

<p align="center">
    <img src="assets/img/p22fig1.png" alt=""  width="800"/>
</p>
<p align="left">
    <em>Figure 1: A contrastive retrieval mechanism.   A query image is encoded and compared with the embeddings of a corpus of reference reports.  The report with the greatest cosine similarity in the shared embedding space is returned as the output.</em>
</p>



## Results 



| Output | Link | 
| ---- | ---- |
| Open Source Code & Documentation | [Github](https://github.com/nhsx/txt-ray-align) |
| Case Study | Awaiting Sign-Off |
| Technical report | [Here](https://github.com/nhsx/txt-ray-align/blob/main/report/TxtRayAlign\_Report\_DZ.pdf) |

||||
|:-|:-|:-|
|<img src="assets/img/simulation_badge_S.png" alt  width="80"/>|<img src="assets/img/Synthetic.png" alt  width="80"/>|<img src="assets/img/machine_learning_badge_S.png" alt  width="80"/>|
